Source idea:
https://x.com/snewmanpv/status/1868464943718416756

"Clearly someone needs to try this at scale ‚Äì pick 1000 published scientific papers at random, ask o1 or o1-pro to look for errors, and see what turns up. I'm going to give it a shot. Anyone interested in helping out? (Incidentally, h/t

[@gibbnicholas](https://x.com/gibbnicholas)

for also noticing that o1-pro can spot the math error in the black plastics paper: [https://x.com/gibbnicholas/status/1867071882303860770‚Ä¶](https://x.com/gibbnicholas/status/1867071882303860770))

![üëÄ]( A 10 page paper caused a panic because of a math error. I was curious if AI would spot the error by just prompting: ‚Äúcarefully check the math in this paper‚Äù especially as the info is not in training data. o1 gets it in a single shot. Should AI checks be standard in science?"

Bonus Objective:
https://x.com/jposhaughnessy/status/1869024669501895035
"Agreed. Another good Use Case for LLMs is to generate a bunch of null set hypothesizes as researchers rarely ask for funding for a thesis they think is wrong and fail to capture insights via negativa


The approach mentioned seems to be focused on simply using the new models alone to evaluate the information in the papers. I'm more interested in training a model to evaluate papers.

The current plan: publicly source papers for evaluation.

For each paper, train several competing models, and identify potential trouble areas. Then define process for evaluating trouble areas.

Once different classes of errors can be identified, we'll then start training specialized models that we can route to in the correct cases.


